#Meeting Summary AlphaCandy 2022-07-28

#####Meeting between Leon (lecturer) and project team members Tim & Robin (Marc couldn't make it)

##Procedure

Leon will offer as many meetings as we want, but we need to organize them ourselves.
If we make a lot of progress this can be even be multiple times a week, or only once in the entire process if we rather 
figure everything out ourselves. The past has shown, however, that the groups often face problems that he could have 
helped to solve very quickly.

The project team is encouraged to write small summaries for the meetings to help everyone stay up to date.
This also makes it easier for Leon to grade the project and usually has a positive effect on the grade, as he can see 
why certain approaches where chosen and whether he advised us to try them.

##Current project state

The idea of the project is to try different (families of) Deep Reinforcement Learning algorithms on the game Candy 
Crush.

Tim explains how the environment for the Candy Crush game is already built and functional. There is also an UI which 
shows the gameboard and the q-values of the different actions.
The Gym is also vectorized (using the OpenAI API) to enable parallelization of training.

Leon asks whether the vectorized environment is parametrized (which is not the case) and says that this may be a good 
way to test smaller (less complex) environments first using e.g. 3 different types of candy and a 4x4 board (instead of 
6 candies, 8x8). Tim adds that it might actually be a nice idea to plot the results for 4x4, 5x5 etc. boards against 
eachother to compare the results.

Leon also suggests that we could have a look at quickroom learning later.

##Next steps

First we want to apply popular DRL algorithms on the game with the expectation that they are not fitting for solving it.
Robin will try out different DQN approaches and Marc Policy Gradients.
Tim tries to solve the game using Monte Carlo Tree Search first, following 
[a basic](https://www.youtube.com/watch?v=UXW2yZndl7U) and 
[the Alpha Zero approach](https://www.youtube.com/watch?v=62nq4Zsn8vc).
He also wants to try out Decision Transformers.

Leon notes that Candy Crush is similar to Alpha Go, but adds uncertainty as random new candies fall onto the gameboard.
To counter this we could introduce a Candy Buffer on top of the actual gameboard, that is already part of the state.

He also thinks that the game is too complex for Policy Gradients or DQN approaches to work (in an efficient way).
One idea could be to reduce complexity by changing the representation of the candies and/or looking more on the local 
level as beneficial actions often depend on the direct neighbours.

Tim comments that it doesn't matter whether we switch candy A with candy B or the other way around since they end up in 
the same space, so we could halve the action space by that.

Following this Leon continues that we could use symmetries on the gameboard to augment our samples to increase the 
performance of the training process. 
Inverting the gameboard on the vertical axis doesn't change the mechanics but doubles the sample size.
Additionally, for the number of points it makes no difference which color a row of candies has, so in the state tensor 
of 8x8x6 (board size x colours of candy) the last dimension could be shuffled again to make up for another * 6! new 
samples, resulting in a total multiplier of 2*6! = 1440 for each sample.

He further argues that trying to use RNNs (as planned in the DQN approaches) probably won't make much sense and could be
neglected for now.
##Questions

Robin mentions that the visualization (using tkinter) doesn't seem to work on Mac (but on Linux and Windows) and whether 
this is a requirement. Leon answers that this isn't the focus of the project and shouldn't lead to point deduction.

Leon concludes by saying that we should just write him a mail if we have further questions or to schedule the next 
meeting.